{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.cnt = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.sum += val * n\n",
    "        self.cnt += n\n",
    "        self.avg = self.sum / self.cnt\n",
    "\n",
    "\n",
    "def accuracy(logits, labels):\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    return torch.sum(preds == labels) / len(labels)\n",
    "\n",
    "\n",
    "def eval_fn(model, loader, device):\n",
    "\n",
    "    score = AverageMeter()\n",
    "    model.eval()\n",
    "\n",
    "    t = tqdm(loader)\n",
    "    with torch.no_grad():  # no gradient needed\n",
    "        for images, labels in t:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            acc = accuracy(outputs, labels)\n",
    "            score.update(acc.item(), images.size(0))\n",
    "\n",
    "            t.set_description('(=> Test) Score: {:.4f}'.format(score.avg))\n",
    "\n",
    "    return score.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, optimizer, criterion, loader, device):\n",
    "    \n",
    "    time_begin = time.time()\n",
    "    score = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    model.train()\n",
    "    time_train = 0\n",
    "\n",
    "    t = tqdm(loader)\n",
    "    for images, labels in t:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = accuracy(logits, labels)\n",
    "        n = images.size(0)\n",
    "        losses.update(loss.item(), n)\n",
    "        score.update(acc.item(), n)\n",
    "\n",
    "        t.set_description('(=> Training) Loss: {:.4f}'.format(losses.avg))\n",
    "\n",
    "    time_train += time.time() - time_begin\n",
    "    print('training time: ' + str(time_train))\n",
    "    return score.avg, losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(k_folds, val_accuracies, train_accuracies, save_folder=\"plots\"):\n",
    "    # Create the save folder if it doesn't exist\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "    # Check if k_folds is 1, create a single plot without subplots\n",
    "    if k_folds == 1:\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        val_scores = val_accuracies[0]\n",
    "        train_scores = train_accuracies[0]\n",
    "\n",
    "        # Plot the accuracy scores for each epoch\n",
    "        epochs = np.arange(1, len(val_scores) + 1)\n",
    "\n",
    "        # Plot validation accuracy\n",
    "        ax.plot(epochs, val_scores, label='Validation')\n",
    "\n",
    "        # Plot training accuracy\n",
    "        ax.plot(epochs, train_scores, label='Training', linestyle='--')\n",
    "\n",
    "        ax.set_title('Accuracy Scores')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax.legend()\n",
    "\n",
    "        # Save the plot\n",
    "        save_path = os.path.join(save_folder, \"accuracy_plot.png\")\n",
    "        plt.savefig(save_path)\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        # For k_folds greater than 1, create subplots\n",
    "        fig, axes = plt.subplots(nrows=k_folds, ncols=1, figsize=(10, 5 * k_folds))\n",
    "\n",
    "        # Plot accuracy scores for each fold\n",
    "        for fold in range(k_folds):\n",
    "            # Extract accuracy scores for the fold\n",
    "            val_scores = val_accuracies[fold]\n",
    "            train_scores = train_accuracies[fold]\n",
    "\n",
    "            # Plot the accuracy scores for each epoch\n",
    "            epochs = np.arange(1, len(val_scores) + 1)\n",
    "\n",
    "            # Plot validation accuracy\n",
    "            axes[fold].plot(epochs, val_scores, label=f'Validation')\n",
    "\n",
    "            # Plot training accuracy\n",
    "            axes[fold].plot(epochs, train_scores, label=f'Training', linestyle='--')\n",
    "\n",
    "            axes[fold].set_title(f'Fold {fold + 1} Accuracy Scores')\n",
    "            axes[fold].set_xlabel('Epoch')\n",
    "            axes[fold].set_ylabel('Accuracy')\n",
    "            axes[fold].legend()\n",
    "\n",
    "        # Adjust layout and save the plot\n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(save_folder, \"accuracy_plots.png\")\n",
    "        plt.savefig(save_path)\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
